<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Koda86 Blog</title>
<link>https://koda86.github.io/</link>
<atom:link href="https://koda86.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Posts on statistics, biomechanics, and other topics.</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Mon, 11 Aug 2025 22:00:00 GMT</lastBuildDate>
<item>
  <title>How Isotropic Scaling Works in Generalized Procrustes Analysis</title>
  <dc:creator>Daniel Koska</dc:creator>
  <link>https://koda86.github.io/posts/2025-08-12-isotropic-scaling-gpa.html</link>
  <description><![CDATA[ 




<section id="how-isotropic-scaling-in-gpa-works" class="level2">
<h2 class="anchored" data-anchor-id="how-isotropic-scaling-in-gpa-works">How Isotropic Scaling in GPA Works</h2>
<details>
<summary>
Click to expand
</summary>
<p>This entry is more of a mental note for my future self than anything else (which I guess most of this blog is tbh).</p>
<p>If you’ve ever worked with Generalized Procrustes Analysis (GPA) in statistical shape modeling, you’ve probably come across the <em>isotropic scaling</em> step. It sounds innocent enough — just scale all coordinates equally — but this step is both powerful and a little sneaky. Let’s unpack what’s really going on.</p>
<section id="where-it-fits-in-gpa" class="level3">
<h3 class="anchored" data-anchor-id="where-it-fits-in-gpa">Where it fits in GPA</h3>
<p>The core idea of GPA is to remove differences that are <em>not</em> actual shape variation — things like translation, rotation, and scale — so that PCA or other analyses reflect pure shape.</p>
<p>For each shape (X) compared to a reference (often the current mean shape), GPA does:</p>
<ol type="1">
<li><strong>Translation:</strong> Move the centroid to the origin.</li>
<li><strong>Isotropic scaling:</strong> Resize uniformly in all directions to match the reference size.</li>
<li><strong>Rotation:</strong> Rotate to minimize the distance to the reference.</li>
</ol>
<p>Then it updates the mean shape and repeats until everything stops changing much.</p>
</section>
<section id="what-isotropic-scaling-really-is" class="level3">
<h3 class="anchored" data-anchor-id="what-isotropic-scaling-really-is">What isotropic scaling really is</h3>
<p>Isotropic scaling means multiplying <strong>all</strong> coordinates by the same scalar (s).<br>
No stretching in one direction, no skewing — every axis gets scaled equally.</p>
<p>Mathematically, if (X ^{k m}) is a translated shape (with (k) points in (m) dimensions, usually (m=2) or (3)), the scaled shape is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_%7B%5Ctext%7Bscaled%7D%7D%20=%20s%20%5C,%20X%0A"></p>
<p>The scalar (s &gt; 0) is chosen so that the scaled shape is as close as possible to the reference (Y).</p>
</section>
<section id="deriving-the-scaling-factor" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-scaling-factor">Deriving the scaling factor</h3>
<p>We want to minimize the squared Procrustes distance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD%5E2(s)%20=%20%5C%7C%5C,%20sX%20-%20Y%20%5C,%5C%7C_F%5E2%0A"></p>
<p>where (||_F) is the Frobenius norm (sum of squared coordinates, square-rooted).</p>
<p>Expanding and differentiating with respect to (s):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD%5E2(s)%20=%20s%5E2%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%20-%202s%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%20+%20%5Csum_%7Bi,j%7D%20Y_%7Bij%7D%5E2%0A"></p>
<p>Set derivative to zero:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A2s%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%20-%202%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%20=%200%0A"></p>
<p>And solve:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%7D%7B%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%7D%0A"></p>
<p>In matrix form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B%5Cmathrm%7Btrace%7D(X%5E%5Cmathsf%7BT%7D%20Y)%7D%7B%5Cmathrm%7Btrace%7D(X%5E%5Cmathsf%7BT%7D%20X)%7D%0A"></p>
</section>
<section id="the-centroid-size-connection" class="level3">
<h3 class="anchored" data-anchor-id="the-centroid-size-connection">The centroid size connection</h3>
<p>If no specific reference is chosen yet (e.g., the very first iteration), shapes are often scaled to <strong>unit centroid size</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20%5Csqrt%7B%5Csum_%7Bi=1%7D%5Ek%20%5Csum_%7Bj=1%7D%5Em%20X_%7Bij%7D%5E2%7D%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B1%7D%7BC%7D%0A"></p>
<p>This prevents large shapes from dominating the mean early on.</p>
</section>
<section id="why-it-matters" class="level3">
<h3 class="anchored" data-anchor-id="why-it-matters">Why it matters</h3>
<p>Without scaling, size differences will dominate PCA.<br>
With isotropic scaling, we remove those size differences, leaving only relative point configurations.<br>
That’s great for <em>pure shape</em> analysis — but it can be risky if size is biologically meaningful.</p>
<p>And here’s the catch: isotropic scaling ties <em>all</em> coordinate axes to a single size factor.<br>
If your sample varies in length or width, this scaling will also change <strong>vertical coordinates</strong>, even if they were identical before.<br>
That can create <em>artificial deformation</em> in anisotropic structures like weight-bearing feet — a subtle source of bias you might not expect.</p>


<!-- -->

</section>
</details></section>

 ]]></description>
  <category>statistical-shape-modeling</category>
  <category>gpa</category>
  <category>scaling</category>
  <guid>https://koda86.github.io/posts/2025-08-12-isotropic-scaling-gpa.html</guid>
  <pubDate>Mon, 11 Aug 2025 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
