[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Daniel Koska\ndkoska@proton.me\nGitHub/koda86\n\nHey there! I‚Äôm currently working as a postdoc at Chemnitz University of Technology.\nMy professional playground includes data analysis, research methods, and stats. My PhD (finished 2023) involved developing methods for adequately characterizing continuous biomechanical data such as joint angles or force curves. Every now and then, I do some machine learning stuff and play with dynamic systems methods.\nI‚Äôm a big supporter of Free and Open Source Software, as well as the idea of open science ‚Äì it‚Äôs the way to go! You‚Äôll usually find me coding in R and Python.\nOff the clock, I switch gears and jam on my bass. üé∏"
  },
  {
    "objectID": "posts/2026-03-01-overlapping_confidence_intervals.html",
    "href": "posts/2026-03-01-overlapping_confidence_intervals.html",
    "title": "When confidence intervals stop meaning what you think they mean",
    "section": "",
    "text": "Every few years, the same statistical advice makes the rounds again: ‚ÄúDon‚Äôt judge differences by whether confidence intervals overlap.‚Äù That advice is correct. It‚Äôs also old. And, at least for normally distributed means, it‚Äôs largely settled science.\nSo if that were the whole story, there wouldn‚Äôt be much left to say.\nBut here‚Äôs the part that isn‚Äôt settled ‚Äî and that, in practice, matters far more.\n\nAsymmetric confidence intervals are everywhere\nIn modern applied research, especially in medicine, epidemiology, and the social sciences, confidence intervals are very often asymmetric. Think of:\n\nOdds ratios\nHazard ratios\nRate ratios\nQuantiles\nBootstrap confidence intervals\nRobust estimators\n\nThese intervals are routinely shown in forest plots, tables, and figures. They are shown correctly, computed correctly, and interpreted correctly with respect to the null value (e.g., HR = 1).\nAnd yet, something subtle happens the moment we start comparing them visually.\n\n\nForest plots get one thing exactly right\nLet‚Äôs be clear about this upfront: forest plots in randomized trials are not the problem.\nIn a standard forest plot, what you see is a contrast ‚Äî a difference, a log-ratio, or some other effect estimate ‚Äî along with its confidence interval. Checking whether that interval includes the null value is a perfectly valid inferential step.\nThis holds regardless of whether the CI is symmetric or asymmetric.\nNo issue there.\n\n\nWhere things quietly go off the rails\nThe problem starts one step later, and it‚Äôs a step we take almost automatically.\nConsider a subgroup analysis in a randomized trial. You see something like this:\nSubgroup A: \\(\\mathrm{HR} = 0.72\\) (95% CI 0.57‚Äì0.92)\nSubgroup B: \\(\\mathrm{HR} = 0.92\\) (95% CI 0.64‚Äì1.31)\n\\(p_{\\text{interaction}} = 0.23\\)\nStatistically, the message is straightforward: there is no evidence that the treatment effect differs between subgroups.\nVisually, however, the message feels different. One interval excludes 1. The other doesn‚Äôt. The intervals look quite different in width and position. It is almost irresistible to think: ‚ÄúIt works here, but not there.‚Äù\nAt that point, no one is explicitly computing CI overlap. No one is writing down a test statistic. But inference is happening anyway ‚Äî by eye.\n\n\nWhy asymmetry breaks the intuition\nWith symmetric confidence intervals for means, the idea of ‚Äúoverlap‚Äù at least has a rough geometric interpretation. You can argue about how conservative it is, but you know what you‚Äôre looking at.\nWith asymmetric intervals, that intuition collapses:\nThe point estimate is no longer centered.\nThe left and right sides of the interval do not have comparable meaning.\nThe visual distance between two intervals depends on scale, transformation, and estimator ‚Äî not just uncertainty.\nTwo asymmetric confidence intervals can overlap substantially and still correspond to a statistically meaningful difference in effects. They can also look strikingly different while the interaction test clearly says ‚Äúno evidence of effect modification.‚Äù\nIn that setting, asking whether the intervals ‚Äúoverlap‚Äù is not just unreliable ‚Äî it‚Äôs ill-defined.\n\n\nThis is not a niche problem\nImportantly, this is not about bad statistics or sloppy analyses.\nYou can find this exact pattern in well-conducted secondary analyses published in journals like JAMA and JAMA Network Open: asymmetric confidence intervals shown side by side, interaction tests reported (and non-significant), and yet a strong visual pull toward subgroup comparisons.\nThe analyses are fine. The plots are fine. The trouble lies entirely in how easily the human visual system turns those plots into informal hypothesis tests.\n\n\nThe real lesson for practice\nSo what should we actually learn from this?\nNot that confidence intervals are useless. Not that forest plots are misleading. And not that everyone is doing statistics wrong.\nThe lesson is simpler and more uncomfortable:\nOnce confidence intervals are asymmetric, ‚Äúoverlap‚Äù is no longer an inferential concept.\nAt that point, visual comparison answers no well-defined statistical question. If you want to know whether two effects differ, you need a confidence interval for their difference (or ratio), or a formal interaction test.\nAnything else is pattern recognition masquerading as inference.\n\n\nA rule that actually helps\nIf you want a single practical rule to take away, it‚Äôs this:\nConfidence intervals can be interpreted against their null value. They cannot, in general, be interpreted against each other ‚Äî especially when they are asymmetric.\nThat rule won‚Äôt make figures prettier. But it will prevent a surprising number of subtle misinterpretations.\nAnd in applied statistics, that‚Äôs usually the best kind of improvement.\n\n\nWhat to do instead (in one sentence)\nIf your real question is ‚ÄúAre these two effects different?‚Äù, compute a confidence interval for the contrast of effects (or run an explicit interaction test). Do not try to answer that question by eyeballing whether two asymmetric confidence intervals overlap."
  },
  {
    "objectID": "posts/2025-08-12-isotropic-scaling-gpa.html",
    "href": "posts/2025-08-12-isotropic-scaling-gpa.html",
    "title": "How Isotropic Scaling Works in Generalized Procrustes Analysis",
    "section": "",
    "text": "This entry is more of a mental note for my future self than anything else (which I guess most of this blog is tbh).\nIf you‚Äôve ever worked with Generalized Procrustes Analysis (GPA) in statistical shape modeling, you‚Äôve probably come across the isotropic scaling step. It sounds innocent enough ‚Äî just scale all coordinates equally ‚Äî but this step is both powerful and a little sneaky. Let‚Äôs unpack what‚Äôs really going on.\n\nWhere it fits in GPA\nThe core idea of GPA is to remove differences that are not actual shape variation ‚Äî things like translation, rotation, and scale ‚Äî so that PCA or other analyses reflect pure shape.\nFor each shape (X) compared to a reference (often the current mean shape), GPA does:\n\nTranslation: Move the centroid to the origin.\nIsotropic scaling: Resize uniformly in all directions to match the reference size.\nRotation: Rotate to minimize the distance to the reference.\n\nThen it updates the mean shape and repeats until everything stops changing much.\n\n\nWhat isotropic scaling really is\nIsotropic scaling means multiplying all coordinates by the same scalar (s).\nNo stretching in one direction, no skewing ‚Äî every axis gets scaled equally.\nMathematically, if (X ^{k m}) is a translated shape (with (k) points in (m) dimensions, usually (m=2) or (3)), the scaled shape is:\n\\[\nX_{\\text{scaled}} = s \\, X\n\\]\nThe scalar (s &gt; 0) is chosen so that the scaled shape is as close as possible to the reference (Y).\n\n\nDeriving the scaling factor\nWe want to minimize the squared Procrustes distance:\n\\[\nD^2(s) = \\|\\, sX - Y \\,\\|_F^2\n\\]\nwhere (||_F) is the Frobenius norm (sum of squared coordinates, square-rooted).\nExpanding and differentiating with respect to (s):\n\\[\nD^2(s) = s^2 \\sum_{i,j} X_{ij}^2 - 2s \\sum_{i,j} X_{ij} Y_{ij} + \\sum_{i,j} Y_{ij}^2\n\\]\nSet derivative to zero:\n\\[\n2s \\sum_{i,j} X_{ij}^2 - 2 \\sum_{i,j} X_{ij} Y_{ij} = 0\n\\]\nAnd solve:\n\\[\ns = \\frac{\\sum_{i,j} X_{ij} Y_{ij}}{\\sum_{i,j} X_{ij}^2}\n\\]\nIn matrix form:\n\\[\ns = \\frac{\\mathrm{trace}(X^\\mathsf{T} Y)}{\\mathrm{trace}(X^\\mathsf{T} X)}\n\\]\n\n\nThe centroid size connection\nIf no specific reference is chosen yet (e.g., the very first iteration), shapes are often scaled to unit centroid size:\n\\[\nC = \\sqrt{\\sum_{i=1}^k \\sum_{j=1}^m X_{ij}^2}\n\\]\nand\n\\[\ns = \\frac{1}{C}\n\\]\nThis prevents large shapes from dominating the mean early on.\n\n\nWhy it matters\nWithout scaling, size differences will dominate PCA.\nWith isotropic scaling, we remove those size differences, leaving only relative point configurations.\nThat‚Äôs great for pure shape analysis ‚Äî but it can be risky if size is biologically meaningful.\nAnd here‚Äôs the catch: isotropic scaling ties all coordinate axes to a single size factor.\nIf your sample varies in length or width, this scaling will also change vertical coordinates, even if they were identical before.\nThat can create artificial deformation in anisotropic structures like weight-bearing feet ‚Äî a subtle source of bias you might not expect."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel Koska",
    "section": "",
    "text": "Welcome to my blog.\nThis is intended to be a collection of ongoing and past projects, papers, coding adventures and whatever else seems worth sharing. This blog‚Äôs running on GitHub Pages and is written using Quarto, (hopefully) making it easy to share code examples. And hey, if you spot something cool, you can dive straight into the GitHub repo and start playing around with it."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Daniel Koska",
    "section": "Latest Posts",
    "text": "Latest Posts\n\n\n\n\n\n\n\n\n\n\nIntuition on reduced variance estimates in pointwise vs.¬†functional analyses\n\n\n\n\n\n\nstatistics\n\n\ntime series\n\n\nfunctional data analysis\n\n\n\n\n\n\n\n\n\nJan 27, 2026\n\n\nDaniel Koska\n\n\n\n\n\n\n\n\n\n\n\n\nWhen confidence intervals stop meaning what you think they mean\n\n\n\n\n\n\nconfidence intervals\n\n\ninference\n\n\nvisualization\n\n\n\nOn asymmetric confidence intervals and the limits of inference by eye\n\n\n\n\n\nJan 3, 2026\n\n\nDaniel Koska\n\n\n\n\n\n\n\n\n\n\n\n\nHow Isotropic Scaling Works in Generalized Procrustes Analysis\n\n\n\n\n\n\nstatistical-shape-modeling\n\n\ngpa\n\n\nscaling\n\n\n\nA deep dive into isotropic scaling in GPA with formulas, intuition, and potential pitfalls.\n\n\n\n\n\nAug 12, 2025\n\n\nDaniel Koska\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#software-contributions",
    "href": "index.html#software-contributions",
    "title": "Daniel Koska",
    "section": "Software Contributions",
    "text": "Software Contributions\n\n\nClick to expand\n\n\nFunctional Bootstrapped Bands (FunBootBand)\n\nBackground story\n\n\nClick to expand\n\nThe whole journey began when I started thinking about ways to adequately characterize differences between joint angles calculated from different measurement systems. In my case, I wanted to compare joint angles from a 3D camera system and an inertial measurement unit system. In terms of choosing appropriate statistics, there are some good reads about what not to do and how to implement methods for discrete data. See, e. g., the highly cited work by Bland and Altman (e.g the papers form 1986, or 2007), who introduced the so called Limits of Agreement (LoA) approach. There‚Äôs much less literature, however, on how to handle continuous differences between two measurement systems.\nOne paper that caught my attention is the article by R√∏slien et al., (2012), which describes a way to expand the LoA method to continuous data by using a functional approach. The paper in a nutshell: Continuous data (aka curves) are approximated using functions to calculate a functional counterpart of the LoA. The problem, however, is that the approach described in the paper is not entirely functional. Rather, the actual calculation of the Functional Limits of Agreement is carried out for each individual point of the previously determined functional curves (for details see the implementation in the utilized R package fda by James Ramsay). Strictly speaking, this turns the whole idea of using functions on its head and results in functional LoA (or more generally: statistical bands) that are likely too narrow (Koska et al., 2023). Applied to my original problem of describing continuous differences between measurement systems, this would mean that the random measurement error is likely underestimated.\nAnother paper that describes a functional approach is the one by Lenhoff et al.¬†(1999), i.e., curves/time series are approximated using functions just like in R√∏islien et al.¬†(2012). The paper is not specifically about quantifying differences between measurement systems, but more generally about the construction of statistical bands (more precisely confidence and prediction bands) for biomechanical curve data. The actual hack here is that the distribution of curves is estimated by bootstrapping the coefficients of the curve functions. This means that bands are no longer calculated pointwise, but the entire curve is included in the calculation. The method in Lenhoff et al., to the best of my knowledge, is based on the work of Olshen, Biden, Wyatt, and Sutherland from the 1980s (see, for example, Sutherland et al., 1988; Olshen et al., 1989).\n\nSutherland, D., Olshen, R., Biden, E., Wyatt, M., 1988. Development of Mature Walking. Mac Keith Press. Olshen, R.A., Biden, E.N., Wyatt, M.P., Sutherland, D.H., 1989. Gait analysis and the bootstrap. Ann. Statist. 17 (4), http://dx.doi.org/10.1214/aos/1176347372.\nOlshen, R.A., Biden, E.N., Wyatt, M.P., Sutherland, D.H., 1989. Gait analysis and the bootstrap. Ann. Statist. 17 (4), http://dx.doi.org/10.1214/aos/1176347372.\n\nAs you can see, the method is not exactly new. I was all the more surprised that I wasn‚Äôt able to find a coded version of the algorithm online. This forced me to implement the algorithm from scratch, which I did using R and the formulas in the appendix of Lenhoff et al.¬†(1999). Extremely helpful at this point was the Matlab code that a former colleague at our institute, Dr.¬†Doris Oriwol, kindly provided me with. Her code allowed me to cross-check my implementation and correct a couple of mistakes. For instance - if I remember correctly - I struggled to implement the correction facor that adjusts the width of the bands to the desired confidence level. Not sure if I‚Äôd gotten it right without Doris‚Äô help. So, full credit to Doris!\n\nAs a side note: I am very grateful for people who have the know-how and the time to review the code. The description in the Lenhoff paper is rather brief, and we were not always 100% sure whether we had correctly implemented the algorithm. Further opinions and possible corrections are very welcome.\n\nI‚Äôm sure other researchers have been here before and would have loved to read a coded version of the algorithm. My hope is that sharing our code is a major pain release in that regard and will lead to more people adopting the method. As indicated by Lenhoff et al., functional statistical bands are not limited to the description of differences between measurement systems, but are useful wherever the variation of curve data needs to be analyzed statistically. This includes a ton of important tasks such as estimating population parameters, indicating precision, assessing statistical significance, comparing groups, forecasting future observations, quantifying uncertainty in predictions etc..\n\nIn the course of implementing the method, I noticed something else: The examined bootstrap methods (including that of R√∏islien) have implemented a naive bootstrap, meaning they assume independence of the curves in the dataset. Accordingly, the papers suggested to include only one curve per subject in the bootstrap. From a methodological point of view, this is somewhat problematic since it ignores the intraindividual variance component. In the context of investigating measurement errors, for instance, this means that the variance across repeated measurements within a person is not taken into account. This may further aggravate the problem of bands being too narrow.\nWe therefore extended the functional bootstrap bands to include a possibility to account for repeated measurements (i.e., dependent curves). This was realized using the two-stage or double bootstrap described in Davison and Hinkley (1997), in which subjects (including all of their curves) are sampled with replacement in the first stage, and one curve per subject is drawn without replacement in the second stage. In addition to sharing our code, this implementation of the two-stage bootstrap is - IMHO - the main contribution of ‚Äòour‚Äô FunBootBand method. A systematic comparison of the method with other methodological approaches for characterizing continuous differences between two measurement systemes (pointwise LoA, Functional LoA, Functional Bootstrapped bands) can be found in in Koska et al.¬†(2023). Here, we analyzed the coverage probabilites of these models in different error scenarios (simulated and real-world data) and found that the FunBootBands showed superior performance.\n\nWhat follows are various versions of the functional bootstrap bands (ongoing development) in different programming languages. The R code has already been published as a (devtools) package, and I plan to add it to CRAN at some point as well. In addition, I‚Äôm currently porting the code to Python. If time permits and I can delve into Julia, this may be next in line. Besides porting the function to different languages to improve the accessibility, my main goal is to increase code efficiency and reduce computation times - after all, bootstrapping is quite a computationally intensive. In R this may be done using RCpp, a C++ version. In Python, there is Cython, which should significantly reduce execution time.\n\n\nFunBootBand R\n\n\nClick to expand\n\nThe FunBootBand package contains a function to generate statistical (prediction or confidence) bands from curve data using a functional approach and bootstrapping.\n\n\n\n\n\n\n\n\n\nThe development version of FunBootBand can be installed from GitHub with:\ndevtools::install_github(‚Äúkoda86/FunBootBand‚Äù)\n\n\nFunBootBand Python\n\n\nClick to expand\n\nThe first alpha-ish version is available, see here: https://github.com/koda86/FunBootBand-python\nMy TODO list still includes a bunch of items, such as extensive testing ‚Ä¶ I‚Äôm getting there. Learned a whole lot about Python in the process as well so far.\n\n\nFunBootBand RCpp\n\n\nClick to expand\n\nOn my TODO list.\n\n\n\nEffort to compress\n\n\nClick to expand\n\nWhile trying out different complexity measures, I stumbled upon the ‚Äòeffort to compress‚Äô (ETC) method introduced in (Nagaraj et al., 2013). ETC is a complexity measure for which code was originally presented as Matlab and Python Code. This repository contains an R implementation of the algorithm.\neffort2compress (GitHub)\nSee also the website of Nithin Nagaraj for the Matlab and Python versions: Website Nagaraj\nPython implementation: Github\n\n\nIdentify duplicates in metadata (DupliCheck)\n\n\nClick to expand\n\n\n\n\n\n\nDupliCheck\n\n\n\nI was recently conducting a meta-analysis to get an idea of the effect of digital physiotherapy for the treatment of various orthopedic diseases. For this, I used Pubmed, Cochrane, BASE (Bielefeld Academic Search Engine) and Embase, which resulted in large csv files containing metadata from the identified scientific articles. This metadata includes paper titles, authors, and the DOI, among others. Not going to talk about the pain of designing adequate search strings and exporting these articles here ‚Ä¶\nBefore deciding which papers to include in the meta-analysis, I needed to identify duplicates, i.e., papers that appeared more than once. This seems like a fairly straightforward job, but, as usual, the devil is in the detail. This probably explains why I wasn‚Äôt able to find a suitable R function for the job (though, to be honest, I didn‚Äôt invest an awful lot of time searching).\nOne obvious candidate, the base function duplicated(), only works on exact copies of a string and fails if even a single character is off. This happens, for example, when author names are written differently across databases. For instance, the author ‚Äú√áelik‚Äù is not the same as ‚ÄúCelik.‚Äù Another issue is that two distinct papers can have identical titles and the same authors (in a different order). This should be discernible from their different DOIs, but DOIs are not always available and often come in various formats. The list of complications goes on, making the task more complex. To spare others from the same pain, I packed everything into a small R package called DupliCheck.\nThe package contains a single function called dupliHunter. dupliHunter takes a data frame with (at least) title, author, and DOI columns. It also includes a threshold parameter to possibly fine-tune the allowed similarity between titles (calculated using stringdist::stringdistmatrix with the Jaccard distance method).\nHere‚Äôs how you can use the package:\n\n\nCode\nlibrary(usethis)\nlibrary(devtools)\n# devtools::install_github(\"koda86/DupliCheck\")\nlibrary(DupliCheck)\n\nload(\"~/tmp/data.RData\") # Example data; load your own data here\n\nduplicates &lt;- dupliHunter(data,\n                            title_col = \"Title\",\n                            author_col = \"Authors\",\n                            doi_col = \"DOI\",\n                            threshold = 0.03)\n\n\nThe current version of the function works decently. I haven‚Äôt quantified Type I and II error rates so far, but my experience from working with several datasets and the feedback from colleagues suggest that errors are minimal. I‚Äôm also optimistic about the function‚Äôs ability to generalize across data from different search engines since the tested files represent data from commonly used sources.\nComputational cost and execution speed were issues with my initial (simplistic) loop-based approach. I did some optimzation that included preprocessing titles to escape special characters, or the use of parallel processing. This reduced the execution time for a 4191-row test file significantly (down from 2856 s to about 952 s). It‚Äôs not lightning fast, but it‚Äôs acceptable for vanilla R code. I hope to port the code to C++ to speed up processing time for future projects. We‚Äôll see ‚Ä¶"
  },
  {
    "objectID": "index.html#disco-diffusion-music-video",
    "href": "index.html#disco-diffusion-music-video",
    "title": "Daniel Koska",
    "section": "Disco Diffusion Music Video",
    "text": "Disco Diffusion Music Video\n\n\nClick to expand\n\nThis post is about a (‚ÄòAI‚Äô-generated) music video that I created for my Band ICTRL. Before I go on with the nerdy details, you should check out the video first:\n\n\n\n\nThe video illustrates our song Dead Horse, which is the first of a bunch of singles we are going to drop this year. The idea to create a video like this was inspired by a Reddit post about a recent video by one of my favorite bands: King Gizzard and the Lizzard Wizard (the song is called Iron Lung). The post provided some insights into how the video was created, e.g.¬†this quote from user No_Stay2400:\n‚ÄúFrom Spod, the guy who made it (in a Reddit post back when it came out): Cheers, yeah it was 2 months of trial and error, and using a bunch of different techniques alongside the newest diffusion programs as they rolled out. It was all mapped out frame by frame to work with the music, trying see if I could illustrate the feeling of the music through the video. At times it feels like driving a bus from the backseat, but is amazing when it all comes together. The hardest thing is steering it away from the generic look that AI art can get. Anyway, thanks.‚Äù\n\nThe main tool that caught my interest was Disco Diffusion, a neat open-source AI tool for generative art. Imagine typing in some text prompt and getting intricate pieces of visual art (for all ‚Äúfuture‚Äù readers of the last sentence: Imagine a world where ‚ÄòAI‚Äô was a new and exciting thing). Quickly, the idea was born to create a low-budget video where the prompts and thus the video sequences fit the lyrics of the song.\n\nDisco Diffusion was developed using Google Colab, which is a free, cloud-based Jupyter notebook environment provided by Google. If you are a data scientist or machine learning practitioner, you have probably heard of it. Being cloud-based, Colab requires no setup or installation. Users can start coding immediately in their web browser. Colab supports Python, so it was easy to read and understand the code. That bein said, there‚Äôs absolutely no need for any programming skills, since there are empty fields for all relevant parameters, prompts etc. in the notebook.\n\nColab also provides access to powerful computational resources, including different GPUs and TPUs, which are essential for training deep learning models. There is a certain amount of free resources (so called compute units) one can use to test the setup and get some early images. That, however, is rather slow and these provided credits will be exhausted soon. So it makes sense to switch to the Colab Pro, which cost me roughly 50 Euros or so for a month. I know there‚Äôs also a local version of Disco Diffusion, but I didn‚Äôt consider my computation ressources enough to make a video in reasonable time, so I abandoned that for the moment. This may be something to investigate more in the future, especially since I‚Äôm not particularly keen on throwing my money towards Google. Well.\nThe Colab notebook itself is pretty much self-explanatory. One has to run a bunch of cells to check GPU status, prepare folders and connect a (cloud) drive, import dependencies and set up runtime devices, define necessary functions and so on. The first interesting part are the diffusion and CLIP model settings. I watched a bunch of Youtube videos such as this one where they explained which settings worked for them. Copying mostly from that, I used a ‚Äú512x512_diffusion_uncond_finetune_008100‚Äù diffusion model and checked the ‚Äòuse_secondary_model‚Äô box.\nThe next big thing are the settings. Here I set the number of steps to 150. this values, I believe, is responsible for the amount of details in a single frame. I further set the picture dimension to [1280, 768]. For all the rest, I stuck with the default settings.\nFor the animation settings, I chose the 2D animation mode, checked the ‚Äòkey_frames‚Äô box, and set ‚Äòmax_frames‚Äô to 3500. I calculated the number of frames (3500) to make the length of the video match the length of the song at the chosen framerate of 12 fps. To create the movement you can see in the video, I changed the following parameters (leaving the rest at default):\n\ninterp_spline = ‚ÄòLinear‚Äô\nangle = ‚Äú0:(1.03)‚Äù\nzoom = ‚Äú0:(1.03)‚Äù\ntranslation_x = ‚Äú0:(0)‚Äù\ntranslation_y = ‚Äú0:(0)‚Äù\ntranslation_z = ‚Äú0:(10.0)‚Äù\n\nInitially, I changed the values for the 3D rotation as well, which caused some hard to pin down errors. Took me way too long to figure that out and caused some real frustrations along the way.\nNext in line was the design of the prompts. First of all, I spent a LOT of time figuring out which style I would like to have. The YouTube video linked above, created by a guy called Doctor Diffusion, used pictures from the Polish artist Zdzis≈Çaw Beksi≈Ñski, famously know for his dystopian surrealism, and the American artist Lisa Frank, who is known for rainbow and neon colors and stylized depictions of animals, including dolphins, pandas, and unicorns. I threw in some Edvard Munch, who‚Äôs paintings I admire since being in the Munch museum in Oslo, and I really liked what I saw. Here are the exact prompts I used:\n\ntext_prompts = {\n0: [‚ÄúA single horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n216: [‚ÄúA single ear by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n300: [‚ÄúA single cloud by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n300: [‚ÄúA burning cloud by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n384: [‚ÄúA single horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n468: [‚ÄúA single running woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n684: [‚ÄúA single vomiting woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n720: [‚ÄúA single beating heart by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n888: [‚ÄúA single race horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1080: [‚ÄúA single curtain by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1164: [‚ÄúA single crow by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1212: [‚ÄúA picture of a broken spine by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1320: [‚ÄúA picture of a torch by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1380: [‚ÄúA single running woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1560: [‚ÄúA single vomiting woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1656: [‚ÄúA single running horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1860: [‚ÄúA single bass player by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n1980: [‚ÄúA single running horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n2100: [‚ÄúA single running woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n2280: [‚ÄúA single vomiting woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n2340: [‚ÄúA picture of a screaming woman in the background by Zdzis≈Çaw Beksi≈Ñski and Edvard Munch, Trending on artstation.‚Äù],\n2400: [‚ÄúA picture of a thunderstorm with fire in the background by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n2460: [‚ÄúA picture of a screaming woman in the background by Zdzis≈Çaw Beksi≈Ñski and Edvard Munch, Trending on artstation.‚Äù],\n2520: [‚ÄúA single vomiting woman by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n2700: [‚ÄúA picture of a horse running through a thunderstorm by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n2940: [‚ÄúA picture of a dying horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n3060: [‚ÄúA picture of a golden ship in the sea by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n3144: [‚ÄúA picture of a dying horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n3204: [‚ÄúA picture of a horse in hell by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n3276: [‚ÄúA picture of a horse by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù],\n3372: [‚ÄúA picture of a rainbow by Zdzis≈Çaw Beksi≈Ñski and Lisa Frank, Trending on artstation.‚Äù]\n}\n\nFor the actual creation of the video, I set the ‚Äòblend‚Äô to 0.5 and always started at the ‚Äòfinal_frame‚Äô when I had to rerun the script. This happened quite a few times, for instance because of errors or when I ran out of computing units.\nFinally, after the animation was, it required some minor editing to overlay the audio track onto the video. Et voil√†, a new video was born."
  },
  {
    "objectID": "index.html#isolating-bass-tracks",
    "href": "index.html#isolating-bass-tracks",
    "title": "Daniel Koska",
    "section": "Isolating Bass Tracks",
    "text": "Isolating Bass Tracks\n\n\nClick to expand\n\nI am a fan Rick Beato‚Äôs YouTube channel, especially the series ‚ÄòWhat Makes This Song Great?‚Äô. Also, I always wondered how he got those isolated instrument tracks, especially the bass tracks. One reason, I assume, is that he‚Äôs a well-copnnected producer with access to separated high-quality tracks directly from the artists or record labels for his deep dives.\nAs for my motivation, I thought it would be helpful to have isolated bass tracks for songs I would like to learn. The problem, though, is that I‚Äôm neither a producer nor well-connected. Also, for much of the music I like, YouTube - despite it‚Äôs sheer endless content on almost every niche topic - does not provide either isolated bass tracks or bass players covering these songs. So I did some searching online and found that there a several ways to isolate tracks for single instruments. These include EQ and filtering, layering and reconstruction, and several audio separation techniques such as spectral editing or phase cancellation.\nThe most convenient way, at least for my purposes, seems to be machine learning and AI-based tools since they require the least amount of audio engineering know-how. My current go-to LLM suggested four options (assuming I need a free option available under Linux):\n\nSpleeter (by Deezer)\nDemucs (by Facebook AI Research)\nOpen-Unmix\nVocalRemover.org (Command-Line Interface)\n\nI decided to give Open-Unmix a try, mainly because of the description: ‚ÄúOpen-Unmix is an open-source tool for music source separation, developed by the Music Technology Group at the Technical University of Madrid. It focuses on separating vocals, bass, and drums from music tracks.‚Äù\nWell, I like open-source solutions and I have an academic background, so that was enough to convince me. As for the cons, Open-Unmix may be less computationally efficient, but that wasn‚Äôt my main concern at this stage. I may do a comparison of different tools in a future post, though. Open-Unmix utilizes a bidirectional LSTM architecture for effective separation and requires FFmpeg for audio processing. Also, like basically all the other tools in the list, Open-Unmix is Python-based. Here are the bash commands I used to set up everything:\n\nInstall Python 3 and essential tools\n\nsudo apt update\nsudo apt install python3 python3-venv python3-pip ffmpeg\npython3 is the Python 3 interpreter. python3-venv is the module to create virtual environments. python3-pip ist the Python package installer and ffmpeg is, well ffmpeg ;-)\n\nCreate and activate a virtual environment\n\ncd ~\npython3 -m venv openunmix_env\nsource openunmix_env/bin/activate\n\nUpgrade pip within the virtual environment\n\npip install --upgrade pip\n\nInstall Open-Unmix and verify the installation\n\npip install openunmix\numx --help\nBy the way, ‚Äòdeactivate‚Äô allows one to leave the virtual environment.\nFor a first test, I chose the song Exoplanet by Mother Engine. If converted this (YouTube video) [https://www.youtube.com/watch?v=MJu7bSh_03I] to an MP3 file and used this bash command:\numx --outdir ~/music/isolated_tracks ~/music/isolated_tracks/Exoplanet.mp3\nfrom which I got this error:\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 518.62 MiB is free. Including non-PyTorch memory, this process has 1.44 GiB memory in use. Of the allocated memory 1.36 GiB is allocated by PyTorch, and 44.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nThis error message indicates that my GPU does not have sufficient memory to process the audio file using Open-Unmix. I therefore switched to CPU Processing (instead of GPU processing).\numx --no-cuda --outdir ~/music/isolated_tracks ~/music/isolated_tracks/Exoplanet.mp3\nThis did the track and created a folder named ‚ÄòExoplanet‚Äô including isolated (*.wav) tracks for bass, drums, vocals and others in the same directory as the original MP3 file.\nHere‚Äôs the isolated bass track for ‚ÄúExoplanet‚Äù by Mother Engine:\nüéß Listen to ‚ÄúExoplanet‚Äù Isolated Bass Track\n\n\n\n\n\nThere are some additional tricks that may help improve the performance for future tries. I could, for instance, ‚Ä¶\n\nOptimize CPU processing\n\nFor instance,\numx --no-cuda --outdir ~/music/isolated_tracks ~/music/isolated_tracks/Exoplanet.mp3 --start 30 --duration 60\nshould processes a 60-second segment starting at 30 seconds into the track. I could also limit the number of targets/tracks to specific instruments (e.g., bass):\numx --no-cuda --outdir ~/music/isolated_tracks --targets bass ~/music/isolated_tracks/Exoplanet.mp3\n\nExplore alternative, faster and less resource-intensive tools such as Spleeter or Demucs.\nOr I could use higher-quality audio files such as WAV, or FLAC instead of compressed formats like MP3. These may yield better separation results and may also reduce processing time due to less decoding overhead."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html",
    "href": "posts/2026-01-27-intuition_pointwise.html",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "",
    "text": "It is well established in the statistical literature that pointwise statistics underestimate uncertainty when applied to time-continuous data, such as curves or trajectories. I have already written about this issue a couple of times ‚Äî most often in the context of pointwise versus functional confidence or prediction bands.\nThe explanation usually given is technically correct but somewhat unsatisfying:\n\nPointwise methods treat time points as independent, even though time series are correlated.\n\nWhile this statement identifies the problem, it often stops one intuition step too early.\nIt does not fully explain why this independence assumption is problematic in practice, nor how it leads to underestimated uncertainty at the level of the entire curve.\nThis post is an attempt to fill that gap ‚Äî deliberately avoiding formulas where possible and focusing instead on the underlying mechanism."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#motivation",
    "href": "posts/2026-01-27-intuition_pointwise.html#motivation",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "",
    "text": "It is well established in the statistical literature that pointwise statistics underestimate uncertainty when applied to time-continuous data, such as curves or trajectories. I have already written about this issue a couple of times ‚Äî most often in the context of pointwise versus functional confidence or prediction bands.\nThe explanation usually given is technically correct but somewhat unsatisfying:\n\nPointwise methods treat time points as independent, even though time series are correlated.\n\nWhile this statement identifies the problem, it often stops one intuition step too early.\nIt does not fully explain why this independence assumption is problematic in practice, nor how it leads to underestimated uncertainty at the level of the entire curve.\nThis post is an attempt to fill that gap ‚Äî deliberately avoiding formulas where possible and focusing instead on the underlying mechanism."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#what-variance-is-actually-trying-to-quantify",
    "href": "posts/2026-01-27-intuition_pointwise.html#what-variance-is-actually-trying-to-quantify",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "What variance is actually trying to quantify",
    "text": "What variance is actually trying to quantify\nAt a conceptual level, variance answers a very simple question:\n\nHow freely can the object I observe fluctuate around its mean?\n\nFor scalar data, this is straightforward: values scatter up and down, and variance quantifies that scatter.\nFor time series, however, the object of interest is not a single value, but a curve.\nAnd curves can fluctuate in very different ways:\n\nby shifting up or down as a whole\n\nby bending smoothly\n\nby changing shape or timing\n\nby exhibiting local noise\n\nThe crucial point is that not all of these fluctuation patterns are independent."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#degrees-of-freedom-as-independent-ways-to-vary",
    "href": "posts/2026-01-27-intuition_pointwise.html#degrees-of-freedom-as-independent-ways-to-vary",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "Degrees of freedom as ‚Äúindependent ways to vary‚Äù",
    "text": "Degrees of freedom as ‚Äúindependent ways to vary‚Äù\nA helpful intuition is to think of degrees of freedom not as a number in a formula, but as:\n\nThe number of independent ways in which the data can meaningfully vary.\n\nIf a curve could vary independently at every time point, pointwise statistics would be appropriate.\nMost real time series, however, do not behave like that."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#a-thought-experiment-curves-that-only-shift-vertically",
    "href": "posts/2026-01-27-intuition_pointwise.html#a-thought-experiment-curves-that-only-shift-vertically",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "A thought experiment: curves that only shift vertically",
    "text": "A thought experiment: curves that only shift vertically\nConsider an extreme but instructive example.\nAssume that all observed curves share exactly the same shape and differ only by a vertical offset:\n\none subject‚Äôs curve is always slightly higher\n\nanother subject‚Äôs curve is always slightly lower\n\nImportantly:\n\nthere is real variability between curves\n\nthis variability can be large\n\nbut it is driven by a single latent factor\nIn other words, the entire curve can move ‚Äî but only in one direction."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#what-pointwise-variance-sees-and-what-it-misses",
    "href": "posts/2026-01-27-intuition_pointwise.html#what-pointwise-variance-sees-and-what-it-misses",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "What pointwise variance sees (and what it misses)",
    "text": "What pointwise variance sees (and what it misses)\nIf we compute pointwise variance across curves at each time point, we will indeed observe scatter:\n\nat every time point, curves differ\n\npointwise variance can be substantial\n\nSo far, nothing is ‚Äúwrong‚Äù.\nThe problem arises when these variances are implicitly interpreted as independent information across time.\nPointwise reasoning silently assumes:\n\nEach time point reflects a different aspect of variability.\n\nBut in the vertical-shift example, this is false:\n\nevery time point reflects the same underlying fluctuation\nno new information is gained by observing additional time points\n\nThe same variability is simply being repeated over time."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#variance-dilution-spreading-one-fluctuation-across-many-points",
    "href": "posts/2026-01-27-intuition_pointwise.html#variance-dilution-spreading-one-fluctuation-across-many-points",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "Variance dilution: spreading one fluctuation across many points",
    "text": "Variance dilution: spreading one fluctuation across many points\nHere is the key intuition.\nThe total amount of variability in the data is fixed.\nIf that variability lives in one coherent direction (e.g.¬†a global vertical shift), then uncertainty about the curve as a whole should be large.\nPointwise methods, however, implicitly:\n\ntreat each time point as an independent degree of freedom\n\ndistribute this single source of variability across many assumed degrees of freedom\n\nthereby making uncertainty at each point look deceptively small\n\nThis is not because variance is computed incorrectly, but because it is assigned to the wrong level of the data structure.\nThe variability belongs to the curve ‚Äî not to individual time points.\n\n## Why this matters for inference\nThis mismatch becomes relevant whenever we ask curve-level questions, for example:\n\nDoes a new curve lie within the expected range?\n\nHow uncertain is the entire trajectory?\n\nDo two functional signals differ in a meaningful way?\n\nPointwise bands answer a weaker question:\n\nIs this value plausible at this specific time point?\n\nFunctional inference addresses a stronger one:\n\nIs the entire curve plausible as a single object?\n\nBy ignoring how tightly time points are coupled, pointwise methods underestimate how often whole curves should be considered unusual."
  },
  {
    "objectID": "posts/2026-01-27-intuition_pointwise.html#optional-illustration",
    "href": "posts/2026-01-27-intuition_pointwise.html#optional-illustration",
    "title": "Intuition on reduced variance estimates in pointwise vs.¬†functional analyses",
    "section": "Optional illustration",
    "text": "Optional illustration\nThe following simple simulation illustrates the idea.\nAll curves differ only by a vertical shift.\nlibrary(tidyverse)\nset.seed(1)\nt &lt;- seq(0, 1, length.out = 100) n &lt;- 30\noffsets &lt;- rnorm(n, sd = 1)\ndf &lt;- purrr::map_dfr(seq_len(n), function(i) { tibble( t = t, y = sin(2 * pi * t) + offsets[i], id = i ) })\ndf %&gt;% ggplot(aes(t, y, group = id)) + geom_line(alpha = 0.5) + stat_summary( aes(group = 1), fun = mean, geom = ‚Äúline‚Äù, linewidth = 1.2 ) + theme_minimal() + labs( title = ‚ÄúCurves differ only by a vertical shift‚Äù, subtitle = ‚ÄúPointwise variance is identical across time but reflects one shared fluctuation‚Äù, x = ‚ÄúTime‚Äù, y = ‚ÄúSignal‚Äù )"
  }
]