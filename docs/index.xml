<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Koda86 Blog</title>
<link>https://koda86.github.io/</link>
<atom:link href="https://koda86.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Posts on statistics, biomechanics, and other topics.</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Mon, 26 Jan 2026 23:00:00 GMT</lastBuildDate>
<item>
  <title>Intuition on reduced variance estimates in pointwise vs. functional analyses</title>
  <dc:creator>Daniel Koska</dc:creator>
  <link>https://koda86.github.io/posts/2026-01-27-intuition_pointwise.html</link>
  <description><![CDATA[ 




<!-- --- -->
<!--   title: "Intuition on reduced variance estimates in pointwise vs. functional analyses" -->
<!-- date: 2026-01-27 -->
<!-- categories: [statistics, time series, functional data analysis] -->
<!-- format: -->
<!--   html: -->
<!--   toc: true -->
<!-- code-fold: true -->
<!-- code-tools: true -->
<!-- --- -->
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>It is well established in the statistical literature that <strong>pointwise statistics underestimate uncertainty when applied to time-continuous data</strong>, such as curves or trajectories. I have already written about this issue a couple of times — most often in the context of <strong>pointwise versus functional confidence or prediction bands</strong>.</p>
<p>The explanation usually given is technically correct but somewhat unsatisfying:</p>
<blockquote class="blockquote">
<p><em>Pointwise methods treat time points as independent, even though time series are correlated.</em></p>
</blockquote>
<p>While this statement identifies the problem, it often stops one intuition step too early.<br>
It does not fully explain <strong>why</strong> this independence assumption is problematic in practice, nor <strong>how</strong> it leads to underestimated uncertainty at the level of the <em>entire curve</em>.</p>
<p>This post is an attempt to fill that gap — deliberately avoiding formulas where possible and focusing instead on the underlying mechanism.</p>
<hr>
</section>
<section id="what-variance-is-actually-trying-to-quantify" class="level2">
<h2 class="anchored" data-anchor-id="what-variance-is-actually-trying-to-quantify">What variance is actually trying to quantify</h2>
<p>At a conceptual level, variance answers a very simple question:</p>
<blockquote class="blockquote">
<p><em>How freely can the object I observe fluctuate around its mean?</em></p>
</blockquote>
<p>For scalar data, this is straightforward: values scatter up and down, and variance quantifies that scatter.</p>
<p>For time series, however, the object of interest is not a single value, but a <strong>curve</strong>.<br>
And curves can fluctuate in very different ways:</p>
<ul>
<li>by shifting up or down as a whole<br>
</li>
<li>by bending smoothly<br>
</li>
<li>by changing shape or timing<br>
</li>
<li>by exhibiting local noise</li>
</ul>
<p>The crucial point is that <strong>not all of these fluctuation patterns are independent</strong>.</p>
<hr>
</section>
<section id="degrees-of-freedom-as-independent-ways-to-vary" class="level2">
<h2 class="anchored" data-anchor-id="degrees-of-freedom-as-independent-ways-to-vary">Degrees of freedom as “independent ways to vary”</h2>
<p>A helpful intuition is to think of degrees of freedom not as a number in a formula, but as:</p>
<blockquote class="blockquote">
<p><strong>The number of independent ways in which the data can meaningfully vary.</strong></p>
</blockquote>
<p>If a curve could vary independently at every time point, pointwise statistics would be appropriate.</p>
<p>Most real time series, however, do not behave like that.</p>
<hr>
</section>
<section id="a-thought-experiment-curves-that-only-shift-vertically" class="level2">
<h2 class="anchored" data-anchor-id="a-thought-experiment-curves-that-only-shift-vertically">A thought experiment: curves that only shift vertically</h2>
<p>Consider an extreme but instructive example.</p>
<p>Assume that all observed curves share exactly the same shape and differ <strong>only by a vertical offset</strong>:</p>
<ul>
<li>one subject’s curve is always slightly higher<br>
</li>
<li>another subject’s curve is always slightly lower</li>
</ul>
<p>Importantly:</p>
<ul>
<li><p>there <em>is</em> real variability between curves<br>
</p></li>
<li><p>this variability can be large<br>
</p></li>
<li><p>but it is driven by <strong>a single latent factor</strong></p>
<p>In other words, the entire curve can move — but only in one direction.</p></li>
</ul>
<hr>
</section>
<section id="what-pointwise-variance-sees-and-what-it-misses" class="level2">
<h2 class="anchored" data-anchor-id="what-pointwise-variance-sees-and-what-it-misses">What pointwise variance sees (and what it misses)</h2>
<p>If we compute pointwise variance across curves at each time point, we will indeed observe scatter:</p>
<ul>
<li>at every time point, curves differ<br>
</li>
<li>pointwise variance can be substantial</li>
</ul>
<p>So far, nothing is “wrong”.</p>
<p>The problem arises when these variances are <strong>implicitly interpreted as independent information across time</strong>.</p>
<p>Pointwise reasoning silently assumes:</p>
<blockquote class="blockquote">
<p><em>Each time point reflects a different aspect of variability.</em></p>
</blockquote>
<p>But in the vertical-shift example, this is false:</p>
<ul>
<li>every time point reflects <strong>the same underlying fluctuation</strong></li>
<li>no new information is gained by observing additional time points</li>
</ul>
<p>The same variability is simply being <strong>repeated over time</strong>.</p>
<hr>
</section>
<section id="variance-dilution-spreading-one-fluctuation-across-many-points" class="level2">
<h2 class="anchored" data-anchor-id="variance-dilution-spreading-one-fluctuation-across-many-points">Variance dilution: spreading one fluctuation across many points</h2>
<p>Here is the key intuition.</p>
<p>The total amount of variability in the data is fixed.<br>
If that variability lives in <strong>one coherent direction</strong> (e.g.&nbsp;a global vertical shift), then uncertainty about the curve as a whole should be <strong>large</strong>.</p>
<p>Pointwise methods, however, implicitly:</p>
<ul>
<li>treat each time point as an independent degree of freedom<br>
</li>
<li>distribute this single source of variability across many assumed degrees of freedom<br>
</li>
<li>thereby making uncertainty at each point look deceptively small</li>
</ul>
<p>This is not because variance is computed incorrectly, but because it is <strong>assigned to the wrong level of the data structure</strong>.</p>
<p>The variability belongs to the curve — not to individual time points.</p>
<hr>
<p>## Why this matters for inference</p>
<p>This mismatch becomes relevant whenever we ask <strong>curve-level questions</strong>, for example:</p>
<ul>
<li><em>Does a new curve lie within the expected range?</em><br>
</li>
<li><em>How uncertain is the entire trajectory?</em><br>
</li>
<li><em>Do two functional signals differ in a meaningful way?</em></li>
</ul>
<p>Pointwise bands answer a weaker question:</p>
<blockquote class="blockquote">
<p><em>Is this value plausible at this specific time point?</em></p>
</blockquote>
<p>Functional inference addresses a stronger one:</p>
<blockquote class="blockquote">
<p><em>Is the entire curve plausible as a single object?</em></p>
</blockquote>
<p>By ignoring how tightly time points are coupled, pointwise methods underestimate how often <strong>whole curves</strong> should be considered unusual.</p>
<hr>
</section>
<section id="optional-illustration" class="level2">
<h2 class="anchored" data-anchor-id="optional-illustration">Optional illustration</h2>
<p>The following simple simulation illustrates the idea.<br>
All curves differ only by a vertical shift.</p>
<p>library(tidyverse)</p>
<p>set.seed(1)</p>
<p>t &lt;- seq(0, 1, length.out = 100) n &lt;- 30</p>
<p>offsets &lt;- rnorm(n, sd = 1)</p>
<p>df &lt;- purrr::map_dfr(seq_len(n), function(i) { tibble( t = t, y = sin(2 * pi * t) + offsets[i], id = i ) })</p>
<p>df %&gt;% ggplot(aes(t, y, group = id)) + geom_line(alpha = 0.5) + stat_summary( aes(group = 1), fun = mean, geom = “line”, linewidth = 1.2 ) + theme_minimal() + labs( title = “Curves differ only by a vertical shift”, subtitle = “Pointwise variance is identical across time but reflects one shared fluctuation”, x = “Time”, y = “Signal” )</p>


<!-- -->

</section>

 ]]></description>
  <category>statistics</category>
  <category>time series</category>
  <category>functional data analysis</category>
  <guid>https://koda86.github.io/posts/2026-01-27-intuition_pointwise.html</guid>
  <pubDate>Mon, 26 Jan 2026 23:00:00 GMT</pubDate>
</item>
<item>
  <title>When confidence intervals stop meaning what you think they mean</title>
  <dc:creator>Daniel Koska</dc:creator>
  <link>https://koda86.github.io/posts/2026-03-01-overlapping_confidence_intervals.html</link>
  <description><![CDATA[ 




<p>Every few years, the same statistical advice makes the rounds again: “Don’t judge differences by whether confidence intervals overlap.” That advice is correct. It’s also old. And, at least for normally distributed means, it’s largely settled science.</p>
<p>So if that were the whole story, there wouldn’t be much left to say.</p>
<p>But here’s the part that isn’t settled — and that, in practice, matters far more.</p>
<section id="asymmetric-confidence-intervals-are-everywhere" class="level3">
<h3 class="anchored" data-anchor-id="asymmetric-confidence-intervals-are-everywhere">Asymmetric confidence intervals are everywhere</h3>
<p>In modern applied research, especially in medicine, epidemiology, and the social sciences, confidence intervals are very often asymmetric. Think of:</p>
<ul>
<li><p>Odds ratios</p></li>
<li><p>Hazard ratios</p></li>
<li><p>Rate ratios</p></li>
<li><p>Quantiles</p></li>
<li><p>Bootstrap confidence intervals</p></li>
<li><p>Robust estimators</p></li>
</ul>
<p>These intervals are routinely shown in forest plots, tables, and figures. They are shown correctly, computed correctly, and interpreted correctly with respect to the null value (e.g., HR = 1).</p>
<p>And yet, something subtle happens the moment we start comparing them visually.</p>
</section>
<section id="forest-plots-get-one-thing-exactly-right" class="level3">
<h3 class="anchored" data-anchor-id="forest-plots-get-one-thing-exactly-right">Forest plots get one thing exactly right</h3>
<p>Let’s be clear about this upfront: forest plots in randomized trials are not the problem.</p>
<p>In a standard forest plot, what you see is a contrast — a difference, a log-ratio, or some other effect estimate — along with its confidence interval. Checking whether that interval includes the null value is a perfectly valid inferential step.</p>
<p>This holds regardless of whether the CI is symmetric or asymmetric.</p>
<p>No issue there.</p>
</section>
<section id="where-things-quietly-go-off-the-rails" class="level3">
<h3 class="anchored" data-anchor-id="where-things-quietly-go-off-the-rails">Where things quietly go off the rails</h3>
<p>The problem starts one step later, and it’s a step we take almost automatically.</p>
<p>Consider a subgroup analysis in a randomized trial. You see something like this:</p>
<p>Subgroup A: <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BHR%7D%20=%200.72"> (95% CI 0.57–0.92)</p>
<p>Subgroup B: <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BHR%7D%20=%200.92"> (95% CI 0.64–1.31)</p>
<p><img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Binteraction%7D%7D%20=%200.23"></p>
<p>Statistically, the message is straightforward: there is no evidence that the treatment effect differs between subgroups.</p>
<p>Visually, however, the message feels different. One interval excludes 1. The other doesn’t. The intervals look quite different in width and position. It is almost irresistible to think: “It works here, but not there.”</p>
<p>At that point, no one is explicitly computing CI overlap. No one is writing down a test statistic. But inference is happening anyway — by eye.</p>
</section>
<section id="why-asymmetry-breaks-the-intuition" class="level3">
<h3 class="anchored" data-anchor-id="why-asymmetry-breaks-the-intuition">Why asymmetry breaks the intuition</h3>
<p>With symmetric confidence intervals for means, the idea of “overlap” at least has a rough geometric interpretation. You can argue about how conservative it is, but you know what you’re looking at.</p>
<p>With asymmetric intervals, that intuition collapses:</p>
<p>The point estimate is no longer centered.</p>
<p>The left and right sides of the interval do not have comparable meaning.</p>
<p>The visual distance between two intervals depends on scale, transformation, and estimator — not just uncertainty.</p>
<p>Two asymmetric confidence intervals can overlap substantially and still correspond to a statistically meaningful difference in effects. They can also look strikingly different while the interaction test clearly says “no evidence of effect modification.”</p>
<p>In that setting, asking whether the intervals “overlap” is not just unreliable — it’s ill-defined.</p>
</section>
<section id="this-is-not-a-niche-problem" class="level3">
<h3 class="anchored" data-anchor-id="this-is-not-a-niche-problem">This is not a niche problem</h3>
<p>Importantly, this is not about bad statistics or sloppy analyses.</p>
<p>You can find this exact pattern in well-conducted secondary analyses published in journals like JAMA and JAMA Network Open: asymmetric confidence intervals shown side by side, interaction tests reported (and non-significant), and yet a strong visual pull toward subgroup comparisons.</p>
<p>The analyses are fine. The plots are fine. The trouble lies entirely in how easily the human visual system turns those plots into informal hypothesis tests.</p>
</section>
<section id="the-real-lesson-for-practice" class="level3">
<h3 class="anchored" data-anchor-id="the-real-lesson-for-practice">The real lesson for practice</h3>
<p>So what should we actually learn from this?</p>
<p>Not that confidence intervals are useless. Not that forest plots are misleading. And not that everyone is doing statistics wrong.</p>
<p>The lesson is simpler and more uncomfortable:</p>
<p>Once confidence intervals are asymmetric, “overlap” is no longer an inferential concept.</p>
<p>At that point, visual comparison answers no well-defined statistical question. If you want to know whether two effects differ, you need a confidence interval for their difference (or ratio), or a formal interaction test.</p>
<p>Anything else is pattern recognition masquerading as inference.</p>
</section>
<section id="a-rule-that-actually-helps" class="level3">
<h3 class="anchored" data-anchor-id="a-rule-that-actually-helps">A rule that actually helps</h3>
<p>If you want a single practical rule to take away, it’s this:</p>
<p>Confidence intervals can be interpreted against their null value. They cannot, in general, be interpreted against each other — especially when they are asymmetric.</p>
<p>That rule won’t make figures prettier. But it will prevent a surprising number of subtle misinterpretations.</p>
<p>And in applied statistics, that’s usually the best kind of improvement.</p>
</section>
<section id="what-to-do-instead-in-one-sentence" class="level3">
<h3 class="anchored" data-anchor-id="what-to-do-instead-in-one-sentence">What to do instead (in one sentence)</h3>
<p>If your real question is “Are these two effects different?”, compute a confidence interval for the contrast of effects (or run an explicit interaction test). Do not try to answer that question by eyeballing whether two asymmetric confidence intervals overlap.</p>


<!-- -->

</section>

 ]]></description>
  <category>confidence intervals</category>
  <category>inference</category>
  <category>visualization</category>
  <guid>https://koda86.github.io/posts/2026-03-01-overlapping_confidence_intervals.html</guid>
  <pubDate>Fri, 02 Jan 2026 23:00:00 GMT</pubDate>
</item>
<item>
  <title>How Isotropic Scaling Works in Generalized Procrustes Analysis</title>
  <dc:creator>Daniel Koska</dc:creator>
  <link>https://koda86.github.io/posts/2025-08-12-isotropic-scaling-gpa.html</link>
  <description><![CDATA[ 




<p>This entry is more of a mental note for my future self than anything else (which I guess most of this blog is tbh).</p>
<p>If you’ve ever worked with Generalized Procrustes Analysis (GPA) in statistical shape modeling, you’ve probably come across the <em>isotropic scaling</em> step. It sounds innocent enough — just scale all coordinates equally — but this step is both powerful and a little sneaky. Let’s unpack what’s really going on.</p>
<section id="where-it-fits-in-gpa" class="level3">
<h3 class="anchored" data-anchor-id="where-it-fits-in-gpa">Where it fits in GPA</h3>
<p>The core idea of GPA is to remove differences that are <em>not</em> actual shape variation — things like translation, rotation, and scale — so that PCA or other analyses reflect pure shape.</p>
<p>For each shape (X) compared to a reference (often the current mean shape), GPA does:</p>
<ol type="1">
<li><strong>Translation:</strong> Move the centroid to the origin.</li>
<li><strong>Isotropic scaling:</strong> Resize uniformly in all directions to match the reference size.</li>
<li><strong>Rotation:</strong> Rotate to minimize the distance to the reference.</li>
</ol>
<p>Then it updates the mean shape and repeats until everything stops changing much.</p>
</section>
<section id="what-isotropic-scaling-really-is" class="level3">
<h3 class="anchored" data-anchor-id="what-isotropic-scaling-really-is">What isotropic scaling really is</h3>
<p>Isotropic scaling means multiplying <strong>all</strong> coordinates by the same scalar (s).<br>
No stretching in one direction, no skewing — every axis gets scaled equally.</p>
<p>Mathematically, if (X ^{k m}) is a translated shape (with (k) points in (m) dimensions, usually (m=2) or (3)), the scaled shape is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_%7B%5Ctext%7Bscaled%7D%7D%20=%20s%20%5C,%20X%0A"></p>
<p>The scalar (s &gt; 0) is chosen so that the scaled shape is as close as possible to the reference (Y).</p>
</section>
<section id="deriving-the-scaling-factor" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-scaling-factor">Deriving the scaling factor</h3>
<p>We want to minimize the squared Procrustes distance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD%5E2(s)%20=%20%5C%7C%5C,%20sX%20-%20Y%20%5C,%5C%7C_F%5E2%0A"></p>
<p>where (||_F) is the Frobenius norm (sum of squared coordinates, square-rooted).</p>
<p>Expanding and differentiating with respect to (s):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD%5E2(s)%20=%20s%5E2%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%20-%202s%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%20+%20%5Csum_%7Bi,j%7D%20Y_%7Bij%7D%5E2%0A"></p>
<p>Set derivative to zero:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A2s%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%20-%202%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%20=%200%0A"></p>
<p>And solve:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%7D%7B%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%7D%0A"></p>
<p>In matrix form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B%5Cmathrm%7Btrace%7D(X%5E%5Cmathsf%7BT%7D%20Y)%7D%7B%5Cmathrm%7Btrace%7D(X%5E%5Cmathsf%7BT%7D%20X)%7D%0A"></p>
</section>
<section id="the-centroid-size-connection" class="level3">
<h3 class="anchored" data-anchor-id="the-centroid-size-connection">The centroid size connection</h3>
<p>If no specific reference is chosen yet (e.g., the very first iteration), shapes are often scaled to <strong>unit centroid size</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20%5Csqrt%7B%5Csum_%7Bi=1%7D%5Ek%20%5Csum_%7Bj=1%7D%5Em%20X_%7Bij%7D%5E2%7D%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B1%7D%7BC%7D%0A"></p>
<p>This prevents large shapes from dominating the mean early on.</p>
</section>
<section id="why-it-matters" class="level3">
<h3 class="anchored" data-anchor-id="why-it-matters">Why it matters</h3>
<p>Without scaling, size differences will dominate PCA.<br>
With isotropic scaling, we remove those size differences, leaving only relative point configurations.<br>
That’s great for <em>pure shape</em> analysis — but it can be risky if size is biologically meaningful.</p>
<p>And here’s the catch: isotropic scaling ties <em>all</em> coordinate axes to a single size factor.<br>
If your sample varies in length or width, this scaling will also change <strong>vertical coordinates</strong>, even if they were identical before.<br>
That can create <em>artificial deformation</em> in anisotropic structures like weight-bearing feet — a subtle source of bias you might not expect.</p>


<!-- -->

</section>

 ]]></description>
  <category>statistical-shape-modeling</category>
  <category>gpa</category>
  <category>scaling</category>
  <guid>https://koda86.github.io/posts/2025-08-12-isotropic-scaling-gpa.html</guid>
  <pubDate>Mon, 11 Aug 2025 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
