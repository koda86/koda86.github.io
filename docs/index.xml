<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Koda86 Blog</title>
<link>https://koda86.github.io/</link>
<atom:link href="https://koda86.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Posts on statistics, biomechanics, and other topics.</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Fri, 02 Jan 2026 23:00:00 GMT</lastBuildDate>
<item>
  <title>When confidence intervals stop meaning what you think they mean</title>
  <dc:creator>Daniel Koska</dc:creator>
  <link>https://koda86.github.io/posts/2026-03-01-overlapping_confidence_intervals.html</link>
  <description><![CDATA[ 




<p>Every few years, the same statistical advice makes the rounds again: “Don’t judge differences by whether confidence intervals overlap.” That advice is correct. It’s also old. And, at least for normally distributed means, it’s largely settled science.</p>
<p>So if that were the whole story, there wouldn’t be much left to say.</p>
<p>But here’s the part that isn’t settled — and that, in practice, matters far more.</p>
<section id="asymmetric-confidence-intervals-are-everywhere" class="level3">
<h3 class="anchored" data-anchor-id="asymmetric-confidence-intervals-are-everywhere">Asymmetric confidence intervals are everywhere</h3>
<p>In modern applied research, especially in medicine, epidemiology, and the social sciences, confidence intervals are very often asymmetric. Think of:</p>
<ul>
<li><p>Odds ratios</p></li>
<li><p>Hazard ratios</p></li>
<li><p>Rate ratios</p></li>
<li><p>Quantiles</p></li>
<li><p>Bootstrap confidence intervals</p></li>
<li><p>Robust estimators</p></li>
</ul>
<p>These intervals are routinely shown in forest plots, tables, and figures. They are shown correctly, computed correctly, and interpreted correctly with respect to the null value (e.g., HR = 1).</p>
<p>And yet, something subtle happens the moment we start comparing them visually.</p>
</section>
<section id="forest-plots-get-one-thing-exactly-right" class="level3">
<h3 class="anchored" data-anchor-id="forest-plots-get-one-thing-exactly-right">Forest plots get one thing exactly right</h3>
<p>Let’s be clear about this upfront: forest plots in randomized trials are not the problem.</p>
<p>In a standard forest plot, what you see is a contrast — a difference, a log-ratio, or some other effect estimate — along with its confidence interval. Checking whether that interval includes the null value is a perfectly valid inferential step.</p>
<p>This holds regardless of whether the CI is symmetric or asymmetric.</p>
<p>No issue there.</p>
</section>
<section id="where-things-quietly-go-off-the-rails" class="level3">
<h3 class="anchored" data-anchor-id="where-things-quietly-go-off-the-rails">Where things quietly go off the rails</h3>
<p>The problem starts one step later, and it’s a step we take almost automatically.</p>
<p>Consider a subgroup analysis in a randomized trial. You see something like this:</p>
<p>Subgroup A: <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BHR%7D%20=%200.72"> (95% CI 0.57–0.92)</p>
<p>Subgroup B: <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7BHR%7D%20=%200.92"> (95% CI 0.64–1.31)</p>
<p><img src="https://latex.codecogs.com/png.latex?p_%7B%5Ctext%7Binteraction%7D%7D%20=%200.23"></p>
<p>Statistically, the message is straightforward: there is no evidence that the treatment effect differs between subgroups.</p>
<p>Visually, however, the message feels different. One interval excludes 1. The other doesn’t. The intervals look quite different in width and position. It is almost irresistible to think: “It works here, but not there.”</p>
<p>At that point, no one is explicitly computing CI overlap. No one is writing down a test statistic. But inference is happening anyway — by eye.</p>
</section>
<section id="why-asymmetry-breaks-the-intuition" class="level3">
<h3 class="anchored" data-anchor-id="why-asymmetry-breaks-the-intuition">Why asymmetry breaks the intuition</h3>
<p>With symmetric confidence intervals for means, the idea of “overlap” at least has a rough geometric interpretation. You can argue about how conservative it is, but you know what you’re looking at.</p>
<p>With asymmetric intervals, that intuition collapses:</p>
<p>The point estimate is no longer centered.</p>
<p>The left and right sides of the interval do not have comparable meaning.</p>
<p>The visual distance between two intervals depends on scale, transformation, and estimator — not just uncertainty.</p>
<p>Two asymmetric confidence intervals can overlap substantially and still correspond to a statistically meaningful difference in effects. They can also look strikingly different while the interaction test clearly says “no evidence of effect modification.”</p>
<p>In that setting, asking whether the intervals “overlap” is not just unreliable — it’s ill-defined.</p>
</section>
<section id="this-is-not-a-niche-problem" class="level3">
<h3 class="anchored" data-anchor-id="this-is-not-a-niche-problem">This is not a niche problem</h3>
<p>Importantly, this is not about bad statistics or sloppy analyses.</p>
<p>You can find this exact pattern in well-conducted secondary analyses published in journals like JAMA and JAMA Network Open: asymmetric confidence intervals shown side by side, interaction tests reported (and non-significant), and yet a strong visual pull toward subgroup comparisons.</p>
<p>The analyses are fine. The plots are fine. The trouble lies entirely in how easily the human visual system turns those plots into informal hypothesis tests.</p>
</section>
<section id="the-real-lesson-for-practice" class="level3">
<h3 class="anchored" data-anchor-id="the-real-lesson-for-practice">The real lesson for practice</h3>
<p>So what should we actually learn from this?</p>
<p>Not that confidence intervals are useless. Not that forest plots are misleading. And not that everyone is doing statistics wrong.</p>
<p>The lesson is simpler and more uncomfortable:</p>
<p>Once confidence intervals are asymmetric, “overlap” is no longer an inferential concept.</p>
<p>At that point, visual comparison answers no well-defined statistical question. If you want to know whether two effects differ, you need a confidence interval for their difference (or ratio), or a formal interaction test.</p>
<p>Anything else is pattern recognition masquerading as inference.</p>
</section>
<section id="a-rule-that-actually-helps" class="level3">
<h3 class="anchored" data-anchor-id="a-rule-that-actually-helps">A rule that actually helps</h3>
<p>If you want a single practical rule to take away, it’s this:</p>
<p>Confidence intervals can be interpreted against their null value. They cannot, in general, be interpreted against each other — especially when they are asymmetric.</p>
<p>That rule won’t make figures prettier. But it will prevent a surprising number of subtle misinterpretations.</p>
<p>And in applied statistics, that’s usually the best kind of improvement.</p>
</section>
<section id="what-to-do-instead-in-one-sentence" class="level3">
<h3 class="anchored" data-anchor-id="what-to-do-instead-in-one-sentence">What to do instead (in one sentence)</h3>
<p>If your real question is “Are these two effects different?”, compute a confidence interval for the contrast of effects (or run an explicit interaction test). Do not try to answer that question by eyeballing whether two asymmetric confidence intervals overlap.</p>


<!-- -->

</section>

 ]]></description>
  <category>confidence intervals</category>
  <category>inference</category>
  <category>visualization</category>
  <guid>https://koda86.github.io/posts/2026-03-01-overlapping_confidence_intervals.html</guid>
  <pubDate>Fri, 02 Jan 2026 23:00:00 GMT</pubDate>
</item>
<item>
  <title>How Isotropic Scaling Works in Generalized Procrustes Analysis</title>
  <dc:creator>Daniel Koska</dc:creator>
  <link>https://koda86.github.io/posts/2025-08-12-isotropic-scaling-gpa.html</link>
  <description><![CDATA[ 




<p>This entry is more of a mental note for my future self than anything else (which I guess most of this blog is tbh).</p>
<p>If you’ve ever worked with Generalized Procrustes Analysis (GPA) in statistical shape modeling, you’ve probably come across the <em>isotropic scaling</em> step. It sounds innocent enough — just scale all coordinates equally — but this step is both powerful and a little sneaky. Let’s unpack what’s really going on.</p>
<section id="where-it-fits-in-gpa" class="level3">
<h3 class="anchored" data-anchor-id="where-it-fits-in-gpa">Where it fits in GPA</h3>
<p>The core idea of GPA is to remove differences that are <em>not</em> actual shape variation — things like translation, rotation, and scale — so that PCA or other analyses reflect pure shape.</p>
<p>For each shape (X) compared to a reference (often the current mean shape), GPA does:</p>
<ol type="1">
<li><strong>Translation:</strong> Move the centroid to the origin.</li>
<li><strong>Isotropic scaling:</strong> Resize uniformly in all directions to match the reference size.</li>
<li><strong>Rotation:</strong> Rotate to minimize the distance to the reference.</li>
</ol>
<p>Then it updates the mean shape and repeats until everything stops changing much.</p>
</section>
<section id="what-isotropic-scaling-really-is" class="level3">
<h3 class="anchored" data-anchor-id="what-isotropic-scaling-really-is">What isotropic scaling really is</h3>
<p>Isotropic scaling means multiplying <strong>all</strong> coordinates by the same scalar (s).<br>
No stretching in one direction, no skewing — every axis gets scaled equally.</p>
<p>Mathematically, if (X ^{k m}) is a translated shape (with (k) points in (m) dimensions, usually (m=2) or (3)), the scaled shape is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX_%7B%5Ctext%7Bscaled%7D%7D%20=%20s%20%5C,%20X%0A"></p>
<p>The scalar (s &gt; 0) is chosen so that the scaled shape is as close as possible to the reference (Y).</p>
</section>
<section id="deriving-the-scaling-factor" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-scaling-factor">Deriving the scaling factor</h3>
<p>We want to minimize the squared Procrustes distance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD%5E2(s)%20=%20%5C%7C%5C,%20sX%20-%20Y%20%5C,%5C%7C_F%5E2%0A"></p>
<p>where (||_F) is the Frobenius norm (sum of squared coordinates, square-rooted).</p>
<p>Expanding and differentiating with respect to (s):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AD%5E2(s)%20=%20s%5E2%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%20-%202s%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%20+%20%5Csum_%7Bi,j%7D%20Y_%7Bij%7D%5E2%0A"></p>
<p>Set derivative to zero:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A2s%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%20-%202%20%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%20=%200%0A"></p>
<p>And solve:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B%5Csum_%7Bi,j%7D%20X_%7Bij%7D%20Y_%7Bij%7D%7D%7B%5Csum_%7Bi,j%7D%20X_%7Bij%7D%5E2%7D%0A"></p>
<p>In matrix form:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B%5Cmathrm%7Btrace%7D(X%5E%5Cmathsf%7BT%7D%20Y)%7D%7B%5Cmathrm%7Btrace%7D(X%5E%5Cmathsf%7BT%7D%20X)%7D%0A"></p>
</section>
<section id="the-centroid-size-connection" class="level3">
<h3 class="anchored" data-anchor-id="the-centroid-size-connection">The centroid size connection</h3>
<p>If no specific reference is chosen yet (e.g., the very first iteration), shapes are often scaled to <strong>unit centroid size</strong>:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AC%20=%20%5Csqrt%7B%5Csum_%7Bi=1%7D%5Ek%20%5Csum_%7Bj=1%7D%5Em%20X_%7Bij%7D%5E2%7D%0A"></p>
<p>and</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As%20=%20%5Cfrac%7B1%7D%7BC%7D%0A"></p>
<p>This prevents large shapes from dominating the mean early on.</p>
</section>
<section id="why-it-matters" class="level3">
<h3 class="anchored" data-anchor-id="why-it-matters">Why it matters</h3>
<p>Without scaling, size differences will dominate PCA.<br>
With isotropic scaling, we remove those size differences, leaving only relative point configurations.<br>
That’s great for <em>pure shape</em> analysis — but it can be risky if size is biologically meaningful.</p>
<p>And here’s the catch: isotropic scaling ties <em>all</em> coordinate axes to a single size factor.<br>
If your sample varies in length or width, this scaling will also change <strong>vertical coordinates</strong>, even if they were identical before.<br>
That can create <em>artificial deformation</em> in anisotropic structures like weight-bearing feet — a subtle source of bias you might not expect.</p>


<!-- -->

</section>

 ]]></description>
  <category>statistical-shape-modeling</category>
  <category>gpa</category>
  <category>scaling</category>
  <guid>https://koda86.github.io/posts/2025-08-12-isotropic-scaling-gpa.html</guid>
  <pubDate>Mon, 11 Aug 2025 22:00:00 GMT</pubDate>
</item>
</channel>
</rss>
